# 딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회 - Wrap Up Report

(주)인공지능팩토리 플랫폼에서 국립과학수사연구원이 주관하는 딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회에 참여한 경험에 대한 회고를 작성합니다 ([대회 링크](https://aifactory.space/task/9197/overview))

## 🟡 대회 개요

### 대회 주제

- 이미지 및 동영상 프레임을 입력하여 식별된 얼굴에 대해, 실제 및 가짜 이미지를 판별하는 분류 예측 모델 개발

### 개발 환경

- Google Colab을 사용하여 진행

### 사용 데이터셋

- 본 대회에서는 학습 및 평가 데이터셋이 제공되지 않음
- 따라서 참가자가 직접 데이터셋을 구성
- 평가 데이터셋
  - 상용 동영상 / 이미지 생성 서비스와 오픈소스 모델 기반의 최신 딥페이크 포함
  - 구형 face swap, lip sync 방식도 혼재
- 본 대회는 추론 결과가 아니라, 모델을 제출하여 자동 추론 방식으로 평가를 수행

## 🔵 수행 절차 및 방법

### 수행 절차

1. Task 분석
2. 데이터셋 탐색
3. 모델 탐색
4. 구성한 데이터셋을 기반으로 선택한 모델 학습
5. 추론 및 결과 제출

## 🔘 수행 결과

> 이번 대회에서는 데이터셋을 직접 구성하기 위해 탐색하는 과정에 집중하게 되었습니다. 그 이후에 활용할 모델을 탐색하고 해당 데이터셋과 모델을 활용하여 학습을 진행했습니다.<br>
> 예상보다 데이터셋 탐색에 시간을 많이 할애했고, 직접 모델을 구현하고 모델 자체를 제출하는 해당 대회의 평가 방식에 적응하는 과정에서 어려움을 겪었습니다. 그로인해 처음 계획보다 많은 시도를 하지 못했습니다. 결과적으로는 baseline 모델보다 약간 높은 score로 대회를 마치게 되었습니다.<br>
> 하지만 본 대회에서 처음으로 직접 데이터셋을 구성하고 학습을 진행하는 과정에서 데이터셋의 중요성을 알게 되었습니다. 그리고 논문을 보고 직접 모델을 구현하는데 어려움을 겪는 과정에서 아직 어떤 능력이 부족한지 깨닫게 되었습니다.

### Task 분석

가장 먼저 대회에서 제공되는 baseline을 통해 학습부터 추론 및 제출까지 이어지는 Task를 분석하였습니다

- 임의의 데이터셋에 대해 모델 학습 진행
- 모델을 제출하여 제출된 서버 내에서 자둥 추론을 통한 평가
  - 평가 데이터셋의 이미지 및 동영상 프레임으로부터 얼굴 인식
  - 해당 얼굴 이미지에 대해 실제인지(0), 가짜인지(1) 예측

### 데이터셋 탐색

- [Faceforensics++](https://github.com/ondyari/FaceForensics)
- [Celeb-DF](https://github.com/yuezunli/celeb-deepfakeforensics)
- [WildDeepfake](https://github.com/OpenTAI/wild-deepfake?tab=readme-ov-file)
- [RedFace](https://github.com/kikyou-220/RedFace)
- ...

위와 같이 딥페이크 탐지 task에 사용되는 다양한 데이터셋을 탐색하였습니다

그리고

- 본 대회에서 사용하는 평가 데이터셋의 구성이 **상용 동영상 / 이미지 생성 서비스와 오픈소스 모델 대부분이 사용되었다는 점
- 입상작에 한해 저작물에 대한 기술 양도양수 계약을 체결하여 국립과학수사연구원에서 활용된다는 점

이 2가지 사실에 초점을 맞춰, 연구 목적으로 생성된 기존 데이터셋보다는, 인터넷을 출처로 하여 실제 딥페이크의 다양성이 반영된 `WildDeepfake` 데이터셋을 선택해보았습니다

<!-- ![WildDeepfake Dataset Sample](https://i.ibb.co/sdS0wbGY/image.png) -->
<img src="https://i.ibb.co/sdS0wbGY/image.png" alt="image" border="0">

아쉬운 점으로는, Colab 무료버전 RAM 한계로 인해 데이터셋(72.8 GB) 전체를 사용하지 못하고 `test` 데이터셋인 약 16만장(Fake: 107003, Real: 58659)을 사용하여 학습을 진행하였습니다. 이로 인해 원래 데이터셋의 정보가 온전히 전달되지 않은 영향인지 baseline에 비해 월등한 성능 향상을 볼 수는 없었습니다.
이후에 데이터셋의 크기가 적은 `RedFace` 데이터셋을 뒤늦게 발견하였지만 학습 및 제출까지 진행해보진 못했습니다. 이 데이터셋은 9개의 상업용 온라인 플랫폼을 활용하여 최신 딥페이크 기술을 통합함으로써 실세계의 블랙박스 시나리오를 효과적으로 시뮬레이션합니다. 다른 데이터셋에 비해 최신 논문이다보니 `WildDeepfake`보다 본 대회의 평가 데이터셋의 구성 방식과 더 유사하지 않았을까 예상되었기에 아쉬움이 남았습니다.

<!-- ![RedFace Datset](https://i.postimg.cc/qBFc46gY/seukeulinsyas-2025-11-23-ojeon-3-02-28.png) -->
<img src='https://i.postimg.cc/qBFc46gY/seukeulinsyas-2025-11-23-ojeon-3-02-28.png' border='0' width=700>

### 모델 탐색

- ViT
- ADDNets(Attention-based Deepfake Detection Networks)
- EfficientNet
- Xception
- ConvNeXt V2
- ...

위와 같이 다양한 모델을 고려해보았습니다

처음에는 기존 딥페이크 탐지 관련 대회에서 수상한 모델인 `Xception`, `EfficientNet`을 시도해보려고 했으나, WildDeepfake 데이터셋을 발견하면서 해당 데이터셋 논문에서 제시하는 `ADDNets`를 사용해보기로 했습니다. 이 모델의 경우 구현체가 존재하지 않아 직접 구현해야 했습니다. 하지만 생각보다 구현 과정에서 어려움을 느꼈고, 대회 기간을 고려하여 빠르게 `ViT`로 학습을 진행해보기로 결정하였습니다.

위의 과정에서 논문을 기반으로 직접 모델을 구현하는 능력이 아직은 부족함을 느꼈고, 앞으로 이 부분에 대해 조금 더 경험이 필요하다고 생각했습니다

### 모델 학습 및 제출 진행

#### - 모델 학습

`ViT` 모델에 대해 `transformers`라이브러리를 활용하여 Fine Tuning을 진행했습니다. Colab 무료버전 GPU의 한정된 사용 시간으로 인해, Classifier와 1개의 layer에 대해 가중치를 업데이트하였습니다. 그럼에도 baseline 대비 약 2% 성능이 향상된 모습을 보였습니다.

앞서 말한 한정된 자원으로 인해 `ViT`보다 조금 더 가벼운 모델을 사용해보기 위해 `ConvNeXt V2` 모델을 사용해보았습니다. `V2` 아키텍처가 직접적으로 딥페이크 탐지 task에 사용된 사례는 아직 드물지만 `ConvNeXt 계열(CNN)`은 딥페이크 탐지에 실제로 사용된 사례가 존재했습니다. 이를 근거로, `ConvNeXt-V2`가 딥페이크 탐지 task에서 유리한 지점이 있지 않을까 판단했습니다. 결과적으로는 baseline 대비 5% 떨어진 성능을 보였습니다. 이는 가중치를 업데이트할 layer를 설정하고 Fine Tuning 과정에서 표현력 제한이나 저수준 특징 학습 부족, 사전 학습 데이터 분포와의 차이 등이 원인이 될 수 있겠습니다.

#### - 결과 제출

학습된 모델의 예측 결과를 파일로 제출하는 것이 아니라, 모델 자체를 제출하는 대회는 처음이다 보니 대회 초중반에 제출 과정에서 시행착오를 생각보다 많이 겪었습니다. 이로 인해 대회 초반에 예상보다 시간을 많이 소비했던 것 같습니다. 다음에 이와 비슷한 방식으로 진행되는 대회가 있다면 그때는 보다 빠르게 적응할 것이라고 생각합니다.

> 추론 및 결과 제출에 대한 코드는 대회 규정상 공유할 수 없었습니다


## 🟢 자체 평가 의견 및 회고

### 잘했던 점

- 한정된 대회 기간 내에 진행이 매끄럽지 않은 부분에 대해 빠른 판단을 내린 것
- 함께 대회를 경험할 사람들을 직접 모집한 점
- 대회를 진행하며 어떠한 선택을 할 때, 논문이나 대회에서 제공되는 정보 등에서 근거를 찾은 점

### 시도했으나 잘 되지 않았던 것

- 논문에 존재하는 모델 구조를 직접 구현하려고 했던 것 ➡️ 구현 능력 향상 필요 🔥
<!-- - LoRA를 적용한 모델을 제출하는 과정에서 제출된 모델을 불러오는 코드가 추론 서버에서 실행될 때 라이브러리 관련 오류가 발생... -->

### 아쉬웠던 점

- 자원 부족으로 인해 선택한 데이터셋 전체를 사용하지 못한 점
- 그리고 이 자원 부족을 제대로 해결하지 못한 점 ➡️ 직접 시도해보진 못했으나 `vast.ai`의 존재를 알게 됨
- 뒤늦게 발견한 데이터셋(RedFace)에 대해 학습을 해보지 못한 점

### 이번 대회를 통해 배운 점 또는 시사점

- 학습 데이터셋이 제공되지 않는 대회는 처음이었고, 이를 직접 구성하기 위해 노력하는 과정에서 학습 및 성능 측면에서 데이터셋의 중요성을 깨달음
- 대회에 적용해볼 수단(데이터셋, 모델 등등)의 근거를, 관련 논문을 탐색하여 찾는 태도를 지니게 됨
- 사용하고 싶은 모델의 구현체가 이미 존재하는지 확인하는 태도와 함께 huggingface 라이브러리 활용이 전보다 익숙해짐


## 출처

- [WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection](https://arxiv.org/abs/2101.01456)
- [Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces](https://arxiv.org/abs/2510.08067)
